use cargo_pretty_test::{fetch::parse_cargo_test_output, parsing::parse_cargo_test};
use insta::assert_display_snapshot as shot;

// cargo test -p integration --no-fail-fast

const STDERR: &str = r###"
    Finished test [unoptimized + debuginfo] target(s) in 0.04s
     Running unittests src/lib.rs (target/debug/deps/integration-81ff55cd01c3cf9e)
error: test failed, to rerun pass `-p integration --lib`
     Running unittests src/main.rs (target/debug/deps/integration-f8e8f8e98a910d1e)
     Running tests/parsing.rs (target/debug/deps/parsing-0bfce6b8ca6a7d2c)
   Doc-tests integration
error: 1 target failed:
    `-p integration --lib`
"###;

const STDOUT: &str = r###"

running 8 tests
test submod::ignore ... ignored, reason
test submod::ignore_without_reason ... ignored
test submod::normal_test ... ok
test submod::panic::panicked ... FAILED
test submod::panic::should_panic - should panic ... ok
test submod::panic::should_panic_but_didnt - should panic ... FAILED
test submod::panic::should_panic_without_reanson - should panic ... ok
test works ... ok

failures:

---- submod::panic::panicked stdout ----
thread 'submod::panic::panicked' panicked at tests/integration/src/lib.rs:9:13:
explicit panic
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace

---- submod::panic::should_panic_but_didnt stdout ----
note: test did not panic as expected

failures:
    submod::panic::panicked
    submod::panic::should_panic_but_didnt

test result: FAILED. 4 passed; 2 failed; 2 ignored; 0 measured; 0 filtered out; finished in 0.00s


running 1 test
test from_main_rs ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s


running 1 test
test from_integration ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s


running 1 test
test tests/integration/src/lib.rs - doc (line 41) ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s
"###;

#[test]
#[allow(clippy::too_many_lines)]
fn serialize_parsed_cargo_test_output() {
    let (tree, stats) = parse_cargo_test_output(STDERR, STDOUT);

    shot!(tree, @r###"
    Generated by cargo-pretty-test
    ├── (FAIL) integration ... (10 tests in 0.00s: ✅ 6; ❌ 2; 🔕 2)
    │   ├── (FAIL) src/lib.rs ... (8 tests in 0.00s: ✅ 4; ❌ 2; 🔕 2)
    │   │   ├── submod
    │   │   │   ├─ 🔕 ignore
    │   │   │   ├─ 🔕 ignore_without_reason
    │   │   │   ├─ ✅ normal_test
    │   │   │   └── panic
    │   │   │       ├─ ❌ panicked
    │   │   │       ├─ ✅ should_panic - should panic
    │   │   │       ├─ ❌ should_panic_but_didnt - should panic
    │   │   │       └─ ✅ should_panic_without_reanson - should panic
    │   │   └─ ✅ works
    │   ├── (OK) src/main.rs ... (1 tests in 0.00s: ✅ 1)
    │   │   └─ ✅ from_main_rs
    │   └── (OK) tests/parsing.rs ... (1 tests in 0.00s: ✅ 1)
    │       └─ ✅ from_integration
    └── (OK) Doc Tests ... (1 tests in 0.00s: ✅ 1)
        └── (OK) integration ... (1 tests in 0.00s: ✅ 1)
            └─ ✅ tests/integration/src/lib.rs - doc (line 41)
    "###);

    shot!(stats, @"Status: FAIL; total 11 tests in 0.00s: 7 passed; 2 failed; 2 ignored; 0 measured; 0 filtered out");

    shot!(parse_cargo_test(STDERR, STDOUT).to_json_string(), @r###"
    {
      "pkgs": {
        "integration": {
          "inner": [
            {
              "runner": {
                "ty": "UnitLib",
                "src": {
                  "src_path": "src/lib.rs",
                  "bin_name": "integration"
                }
              },
              "info": {
                "raw": "running 8 tests\ntest submod::ignore ... ignored, reason\ntest submod::ignore_without_reason ... ignored\ntest submod::normal_test ... ok\ntest submod::panic::panicked ... FAILED\ntest submod::panic::should_panic - should panic ... ok\ntest submod::panic::should_panic_but_didnt - should panic ... FAILED\ntest submod::panic::should_panic_without_reanson - should panic ... ok\ntest works ... ok\n\nfailures:\n\n---- submod::panic::panicked stdout ----\nthread 'submod::panic::panicked' panicked at tests/integration/src/lib.rs:9:13:\nexplicit panic\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n---- submod::panic::should_panic_but_didnt stdout ----\nnote: test did not panic as expected\n\nfailures:\n    submod::panic::panicked\n    submod::panic::should_panic_but_didnt\n\ntest result: FAILED. 4 passed; 2 failed; 2 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n\n",
                "stats": {
                  "ok": false,
                  "total": 8,
                  "passed": 4,
                  "failed": 2,
                  "ignored": 2,
                  "measured": 0,
                  "filtered_out": 0,
                  "finished_in": {
                    "secs": 0,
                    "nanos": 0
                  }
                },
                "parsed": {
                  "head": "running 8 tests",
                  "tree": [
                    "test submod::ignore ... ignored, reason",
                    "test submod::ignore_without_reason ... ignored",
                    "test submod::normal_test ... ok",
                    "test submod::panic::panicked ... FAILED",
                    "test submod::panic::should_panic - should panic ... ok",
                    "test submod::panic::should_panic_but_didnt - should panic ... FAILED",
                    "test submod::panic::should_panic_without_reanson - should panic ... ok",
                    "test works ... ok"
                  ],
                  "detail": "failures:\n\n---- submod::panic::panicked stdout ----\nthread 'submod::panic::panicked' panicked at tests/integration/src/lib.rs:9:13:\nexplicit panic\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n---- submod::panic::should_panic_but_didnt stdout ----\nnote: test did not panic as expected\n\nfailures:\n    submod::panic::panicked\n    submod::panic::should_panic_but_didnt"
                }
              }
            },
            {
              "runner": {
                "ty": "UnitBin",
                "src": {
                  "src_path": "src/main.rs",
                  "bin_name": "integration"
                }
              },
              "info": {
                "raw": "running 1 test\ntest from_main_rs ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n\n",
                "stats": {
                  "ok": true,
                  "total": 1,
                  "passed": 1,
                  "failed": 0,
                  "ignored": 0,
                  "measured": 0,
                  "filtered_out": 0,
                  "finished_in": {
                    "secs": 0,
                    "nanos": 0
                  }
                },
                "parsed": {
                  "head": "running 1 test",
                  "tree": [
                    "test from_main_rs ... ok"
                  ],
                  "detail": ""
                }
              }
            },
            {
              "runner": {
                "ty": "Tests",
                "src": {
                  "src_path": "tests/parsing.rs",
                  "bin_name": "parsing"
                }
              },
              "info": {
                "raw": "running 1 test\ntest from_integration ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n\n",
                "stats": {
                  "ok": true,
                  "total": 1,
                  "passed": 1,
                  "failed": 0,
                  "ignored": 0,
                  "measured": 0,
                  "filtered_out": 0,
                  "finished_in": {
                    "secs": 0,
                    "nanos": 0
                  }
                },
                "parsed": {
                  "head": "running 1 test",
                  "tree": [
                    "test from_integration ... ok"
                  ],
                  "detail": ""
                }
              }
            }
          ],
          "stats": {
            "ok": false,
            "total": 10,
            "passed": 6,
            "failed": 2,
            "ignored": 2,
            "measured": 0,
            "filtered_out": 0,
            "finished_in": {
              "secs": 0,
              "nanos": 0
            }
          }
        },
        "Doc Tests": {
          "inner": [
            {
              "runner": {
                "ty": "Doc",
                "src": {
                  "src_path": "integration",
                  "bin_name": "integration"
                }
              },
              "info": {
                "raw": "running 1 test\ntest tests/integration/src/lib.rs - doc (line 41) ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n",
                "stats": {
                  "ok": true,
                  "total": 1,
                  "passed": 1,
                  "failed": 0,
                  "ignored": 0,
                  "measured": 0,
                  "filtered_out": 0,
                  "finished_in": {
                    "secs": 0,
                    "nanos": 0
                  }
                },
                "parsed": {
                  "head": "running 1 test",
                  "tree": [
                    "test tests/integration/src/lib.rs - doc (line 41) ... ok"
                  ],
                  "detail": ""
                }
              }
            }
          ],
          "stats": {
            "ok": true,
            "total": 1,
            "passed": 1,
            "failed": 0,
            "ignored": 0,
            "measured": 0,
            "filtered_out": 0,
            "finished_in": {
              "secs": 0,
              "nanos": 0
            }
          }
        }
      }
    }
    "###);
}
